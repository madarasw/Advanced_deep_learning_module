{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP2wzuxOjRaSuuD14SM9oR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madarasw/Advanced_deep_learning_module/blob/main/248084L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fVNRzYxBBYq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a8f511-c2c6-40d2-adc2-05affa3a3af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "opaxUSctGjUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Define the Neurone class\n",
        "# remember: np.float32\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, activation):\n",
        "\n",
        "        # Parameters\n",
        "        self.weights = np.empty(1, dtype=np.float32)\n",
        "        self.bias = 0\n",
        "        self.activation = activation # Activation function\n",
        "\n",
        "        # Inputs and Outputs\n",
        "        self.inputs = np.empty(1, dtype=np.float32)\n",
        "        self.a = np.float32(1.0)\n",
        "        self.z = np.float32(1.0)\n",
        "\n",
        "        # Derivatives\n",
        "        self.dz_dw = []\n",
        "        self.dz_db = 1\n",
        "\n",
        "    # Activation functions\n",
        "    def relu(self, x):\n",
        "        return x if x > 0 else 0\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights\n",
        "\n",
        "    def set_bias(self, bias):\n",
        "        self.bias = bias\n",
        "\n",
        "    # Forward pass\n",
        "    def forward_prop(self, inputs):\n",
        "        self.inputs = np.array(inputs, dtype=np.float32)\n",
        "        self.a = np.dot(self.weights, self.inputs) + self.bias  # weighted sum + bias\n",
        "        if self.activation == 'ReLU':\n",
        "            self.z = self.relu(self.a)\n",
        "        elif self.activation == 'Sigmoid':\n",
        "            self.z = self.sigmoid(self.a)\n",
        "        else:\n",
        "            self.z = self.a  # linear\n",
        "\n",
        "        return self.z\n",
        "\n",
        "    def get_activation_gradient(self):\n",
        "        dz_da = 1\n",
        "        if self.activation == \"ReLU\":\n",
        "          if self.z <= 0:\n",
        "            dz_da = 0\n",
        "          else:\n",
        "            dz_da = 1\n",
        "        elif self.activation == \"Sigmoid\":\n",
        "          dz_da = self.z * (1 - self.z)\n",
        "        else:\n",
        "          dz_da = 1                          #for linear activation\n",
        "\n",
        "        return dz_da\n",
        "\n",
        "    # backward pass\n",
        "    def backward_pass(self):\n",
        "\n",
        "        dz_da = self.get_activation_gradient()  #Scalaer value\n",
        "        da_dw = self.inputs                     #vector - eg: [z1,z2,z3]\n",
        "\n",
        "        # gradients of weights\n",
        "        self.dz_dw = []\n",
        "        for dadw in da_dw:\n",
        "            self.dz_dw.append(dz_da*dadw)       #vector\n",
        "\n",
        "        # gradients of bias\n",
        "        da_db = 1\n",
        "        self.dz_db = dz_da*da_db                #Scalar value\n",
        "\n",
        "        # d_output_by_d_input\n",
        "        da_dinput = self.weights                #vector - eg: [w1,w2,w3]\n",
        "        dz_din = []\n",
        "        for dadinput in da_dinput:\n",
        "            dz_din.append(dz_da*dadinput)\n",
        "        return dz_din                           #vector\n",
        "\n",
        "    def get_w_grad(self):\n",
        "        return self.dz_dw\n",
        "\n",
        "    def get_b_grad(self):\n",
        "        return self.dz_db"
      ],
      "metadata": {
        "id": "8IN9DOIcGYdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "'''\n",
        "Testing Neuron class\n",
        "====================\n",
        "\n",
        "weights = [2,-1,6]\n",
        "bias = 8\n",
        "activation = 'relu'\n",
        "\n",
        "my_neu = Neuron(activation)\n",
        "\n",
        "my_neu.set_weights(weights)\n",
        "my_neu.set_bias(bias)\n",
        "\n",
        "my_neu.forward_prop([2,-1,3])\n",
        "my_neu.backward_pass(4)\n",
        "\n",
        "w = my_neu.get_w_grad()\n",
        "b = my_neu.get_b_grad()\n",
        "\n",
        "#print(w)\n",
        "#print(b)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qIP3xyZ25u7h",
        "outputId": "6bfe91a9-b788-4336-98c6-045cdcdd98db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nTesting Neuron class\\n====================\\n\\nweights = [2,-1,6]\\nbias = 8\\nactivation = 'relu'\\n\\nmy_neu = Neuron(activation)\\n\\nmy_neu.set_weights(weights)\\nmy_neu.set_bias(bias)\\n\\nmy_neu.forward_prop([2,-1,3])\\nmy_neu.backward_pass(4)\\n\\nw = my_neu.get_w_grad()\\nb = my_neu.get_b_grad()\\n\\n#print(w)\\n#print(b)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write the Layer class\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, Label, N, activation):\n",
        "\n",
        "        # parameters\n",
        "        self.label = Label\n",
        "        self.N = N # layer size (Number of neurons in the layer)\n",
        "        self.activation = activation\n",
        "        self.global_derivative = 1\n",
        "\n",
        "        # neurons\n",
        "        self.neurons = []\n",
        "        self.add_neurons()\n",
        "\n",
        "        # layer output\n",
        "        self.z = []\n",
        "\n",
        "    def add_neurons(self):\n",
        "        for n in range(self.N):\n",
        "          new_neuron = Neuron(self.activation)\n",
        "          self.neurons.append(new_neuron)\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        for n in range(self.N):\n",
        "          w = weights[:, n:n+1]\n",
        "          weights_ = [item for sublist in w for item in sublist] # remove sublists\n",
        "          self.neurons[n].set_weights(weights_)\n",
        "\n",
        "    def set_bias(self, bias):\n",
        "        for n in range(self.N):\n",
        "          self.neurons[n].set_bias(bias[n])\n",
        "\n",
        "    def forward_prop(self, inputs):\n",
        "        for neuron in self.neurons:\n",
        "            self.z.append(neuron.forward_prop(inputs))\n",
        "\n",
        "        return self.z\n",
        "\n",
        "    def backward_pass(self, global_derivative):\n",
        "        local_derivative = []\n",
        "        self.global_derivative = global_derivative\n",
        "        for n in range(self.N):\n",
        "            gd = self.neurons[n].backward_pass()  #vector\n",
        "            local_derivative.append(gd)           #matrix\n",
        "\n",
        "        dj_dz_lower = np.matmul(self.global_derivative, local_derivative)\n",
        "        return dj_dz_lower\n",
        "\n",
        "    def get_w_gradients(self):\n",
        "        dj_dw = []\n",
        "        for n in range(len(self.neurons)):\n",
        "            dz_dw_single = self.neurons[n].get_w_grad() #vector\n",
        "            dj_dw_single = []\n",
        "            for element in dz_dw_single:\n",
        "                dj_dw_single.append(self.global_derivative[n] * element)\n",
        "            dj_dw.append(dj_dw_single) #matrix\n",
        "\n",
        "        return dj_dw\n",
        "\n",
        "    def get_b_gradients(self):\n",
        "        dj_db = []\n",
        "        for n in range(len(self.neurons)):\n",
        "            local_derivative = self.neurons[n].get_b_grad()\n",
        "            dj_db.append(self.global_derivative[n] * local_derivative)\n",
        "\n",
        "        return dj_db\n",
        "\n",
        "##########"
      ],
      "metadata": {
        "id": "4cFYs4NrGSBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Testing Layer class\n",
        "====================\n",
        "\n",
        "l = 0\n",
        "layer_sizes = [3,2,1]\n",
        "layer_activations = ['ReLU', 'ReLU', 'ReLU', 'Sigmoid']\n",
        "weights = np.array([[2,6,2],[1,1,8],[4,2,6]], dtype=np.float32)\n",
        "bias = np.array([1,8,4], dtype=np.float32)\n",
        "\n",
        "layer = Layer(f\"Layer_{l+1}\", layer_sizes[l], layer_activations[l])\n",
        "layer.set_weights(weights)\n",
        "layer.set_bias(bias)\n",
        "layer.forward_prop([2,-1,3])\n",
        "# layer.backward_pass(4)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "collapsed": true,
        "id": "XCzus7MwUv_e",
        "outputId": "c813872d-c25b-4fcb-cb46-37bac20d4e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTesting Layer class\\n====================\\n\\nl = 0\\nlayer_sizes = [3,2,1]\\nlayer_activations = [\\'ReLU\\', \\'ReLU\\', \\'ReLU\\', \\'Sigmoid\\']\\nweights = np.array([[2,6,2],[1,1,8],[4,2,6]], dtype=np.float32)\\nbias = np.array([1,8,4], dtype=np.float32)\\n\\nlayer = Layer(f\"Layer_{l+1}\", layer_sizes[l], layer_activations[l])\\nlayer.set_weights(weights)\\nlayer.set_bias(bias)\\nlayer.forward_prop([2,-1,3])\\n# layer.backward_pass(4)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the NeuralNetwork Class\n",
        "\n",
        "class neuralNetwork:\n",
        "\n",
        "    def __init__(self, network_depth, layer_sizes, layer_activations):\n",
        "        self.depth = network_depth\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.layer_activations = layer_activations\n",
        "        self.layers = []\n",
        "        self.create_layers()\n",
        "        self.loss = 0\n",
        "\n",
        "\n",
        "    def create_layers(self):\n",
        "        for l in range(self.depth):\n",
        "          # set random values for initial parameters\n",
        "          layer = Layer(f\"Layer_{l+1}\", self.layer_sizes[l], self.layer_activations[l])\n",
        "          self.layers.append(layer)\n",
        "\n",
        "    def set_weights(self, weight_vector):\n",
        "        for l in range(self.depth):\n",
        "            self.layers[l].set_weights(weight_vector[l])\n",
        "\n",
        "    def set_bias(self, bias_vector):\n",
        "        for l in range(self.depth):\n",
        "            self.layers[l].set_bias(bias_vector[l])\n",
        "\n",
        "    def forward_prop(self, input):\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer.forward_prop(input)\n",
        "            input = layer_output\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_and_binary_cross_entropy_derivative(self, y_true, y_pred):\n",
        "        # Derivative of Sigmoid and Cross entropy loss together\n",
        "        dJ_dz = y_pred - y_true\n",
        "        return dJ_dz\n",
        "\n",
        "\n",
        "    def backward_prop(self, y_label):\n",
        "        # Calculate dj_dz\n",
        "        last_layer_output = self.layers[-1].z\n",
        "        sigmoid_output = self.sigmoid(last_layer_output[0])\n",
        "\n",
        "        #dJ_da_final\n",
        "        global_derivative = [self.sigmoid_and_binary_cross_entropy_derivative(y_label, sigmoid_output)] # dj_dz_final\n",
        "\n",
        "        #backpropagation\n",
        "        for layer in reversed(self.layers):\n",
        "            global_derivative = layer.backward_pass(global_derivative)\n",
        "\n",
        "\n",
        "    def get_w_gradients(self):\n",
        "        w_gradients = []\n",
        "        for layer in self.layers:\n",
        "            w_gradients.append(layer.get_w_gradients())\n",
        "        return w_gradients\n",
        "\n",
        "    def get_b_gradients(self):\n",
        "        b_gradients = []\n",
        "        for layer in self.layers:\n",
        "            b_gradients.append(layer.get_b_gradients())\n",
        "        return b_gradients"
      ],
      "metadata": {
        "id": "Onx693fUbm3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ------------------------"
      ],
      "metadata": {
        "id": "WeyqgTNsj2El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Processing\n",
        "\n",
        "def input_processing(index):\n",
        "  processed = np.array([d for d in str(index)], dtype=np.float32)\n",
        "  processed = processed / 9.0\n",
        "  return processed\n",
        "\n",
        "def read_weights():\n",
        "  file_path = '/content/drive/My Drive/Colab Notebooks/Advanced_Deep_Learning/W.csv'\n",
        "  weights = pd.read_csv(file_path, header=None, float_precision='round_trip')\n",
        "  weights.drop(weights.columns[[0]], axis=1, inplace=True)\n",
        "  weights = np.array(weights, dtype=np.float32)\n",
        "  return weights\n",
        "\n",
        "def read_bias():\n",
        "  file_path = '/content/drive/My Drive/Colab Notebooks/Advanced_Deep_Learning/b.csv'\n",
        "  bias = pd.read_csv(file_path, header=None, float_precision='round_trip')\n",
        "  bias.drop(bias.columns[[0]], axis=1, inplace=True)\n",
        "  bias = np.array(bias, dtype=np.float32)\n",
        "  return bias"
      ],
      "metadata": {
        "id": "Meas0YqxGIHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read weights and biases\n",
        "weights = read_weights()\n",
        "biases = read_bias()\n",
        "\n",
        "# Input and expected output preprocessing\n",
        "index = 248084\n",
        "input = input_processing(index)\n",
        "y_label = 0 #Index number 248084 is not a prime number\n",
        "\n",
        "# Specify the network properties\n",
        "network_depth = 4 # excluding input layer\n",
        "layer_sizes = [64,32,16,1]\n",
        "\n",
        "layer_activations = ['ReLU', 'ReLU', 'ReLU', 'Linear'] # set last layer activation as linear to get the logit\n",
        "\n",
        "# assign values to the weight vector and bias vector\n",
        "weight_vector = []\n",
        "bias_vector = []\n",
        "\n",
        "weight_vector.append(weights[0:6,:64])\n",
        "weight_vector.append(weights[6:70,:32])\n",
        "weight_vector.append(weights[70:102,:16])\n",
        "weight_vector.append(weights[102:118,:1])\n",
        "\n",
        "for i in range(network_depth):\n",
        "    bias_vector.append(biases[i, :layer_sizes[i]])"
      ],
      "metadata": {
        "id": "z5dvJd48cqHN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the neural network using the neuralNetwork class\n",
        "primeNeuralNetwork = neuralNetwork(network_depth, layer_sizes, layer_activations)\n",
        "\n",
        "# Set weights and biases of the neural network\n",
        "primeNeuralNetwork.set_weights(weight_vector)\n",
        "primeNeuralNetwork.set_bias(bias_vector)\n",
        "\n",
        "# Forward propagation using the input\n",
        "primeNeuralNetwork.forward_prop(input)\n",
        "# Backword propagation using the y label for the given input\n",
        "primeNeuralNetwork.backward_prop(y_label)\n",
        "\n",
        "# Get gradients for weights and bias for each neuron\n",
        "w_gradients = primeNeuralNetwork.get_w_gradients()\n",
        "b_gradients = primeNeuralNetwork.get_b_gradients()\n",
        "\n",
        "#print('---------------------------- FINAL RESULT ----------------------------')\n",
        "#print(w_gradients)\n",
        "#print(b_gradients)\n",
        "\n",
        "# Write output in to csv files\n",
        "csv_filename = '/content/drive/My Drive/Colab Notebooks/Advanced_Deep_Learning/dw_just.csv'\n",
        "with open(csv_filename, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for g in w_gradients:\n",
        "      transposed = np.array(g).T\n",
        "      writer.writerows(transposed)\n",
        "\n",
        "csv_filename = '/content/drive/My Drive/Colab Notebooks/Advanced_Deep_Learning/db?_just.csv'\n",
        "with open(csv_filename, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(b_gradients)"
      ],
      "metadata": {
        "id": "0DzUS27u3ndk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "TESTING\n",
        "=======================\n",
        "'''\n",
        "\n",
        "def read_weights():\n",
        "  file_path = '/content/drive/My Drive/Colab Notebooks/Advanced_Deep_Learning/W_test.csv'\n",
        "  weights = pd.read_csv(file_path, header=None, float_precision='round_trip')\n",
        "  weights.drop(weights.columns[[0]], axis=1, inplace=True)\n",
        "  weights = np.array(weights, dtype=np.float32)\n",
        "  return weights\n",
        "\n",
        "def read_bias():\n",
        "  file_path = '/content/drive/My Drive/Colab Notebooks/Advanced_Deep_Learning/b_test.csv'\n",
        "  bias = pd.read_csv(file_path, header=None, float_precision='round_trip')\n",
        "  bias.drop(bias.columns[[0]], axis=1, inplace=True)\n",
        "  bias = np.array(bias, dtype=np.float32)\n",
        "  return bias\n",
        "\n",
        "\n",
        "weights = read_weights()\n",
        "biases = read_bias()\n",
        "\n",
        "\n",
        "network_depth = 3 # excluding input layer\n",
        "layer_sizes = [3,2,1]\n",
        "layer_activations = ['ReLU', 'ReLU', 'Linear']\n",
        "\n",
        "weight_vector = []\n",
        "bias_vector = []\n",
        "weight_vector.append(weights[0:3,:3])\n",
        "weight_vector.append(weights[3:6,:2])\n",
        "weight_vector.append(weights[6:8,:1])\n",
        "\n",
        "for i in range(network_depth):\n",
        "    bias_vector.append(biases[i, :layer_sizes[i]])\n",
        "\n",
        "input = [1, 2, 1]\n",
        "y_label = 0\n",
        "\n",
        "# Create the neural network using the neuralNetwork class\n",
        "primeNeuralNetwork = neuralNetwork(network_depth, layer_sizes, layer_activations)\n",
        "\n",
        "# Set weights and biases of the neural network\n",
        "primeNeuralNetwork.set_weights(weight_vector)\n",
        "primeNeuralNetwork.set_bias(bias_vector)\n",
        "\n",
        "# Forward propagation using the input\n",
        "primeNeuralNetwork.forward_prop(input)\n",
        "# Backword propagation using the y label for the given input\n",
        "primeNeuralNetwork.backward_prop(y_label)\n",
        "\n",
        "# Get gradients for weights and bias for each neuron\n",
        "w_gradients = primeNeuralNetwork.get_w_gradients()\n",
        "b_gradients = primeNeuralNetwork.get_b_gradients()\n",
        "\n",
        "print('---------------------------- FINAL RESULT ----------------------------')\n",
        "print(w_gradients)\n",
        "print(b_gradients)"
      ],
      "metadata": {
        "id": "Iz2CEX293GXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcc3056-b252-4b00-d6cc-b4e11d1e21c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1., 2., 5.],\n",
            "       [5., 7., 8.],\n",
            "       [4., 5., 3.]], dtype=float32), array([[2., 1.],\n",
            "       [5., 8.],\n",
            "       [4., 2.]], dtype=float32), array([[ 1.03],\n",
            "       [-1.  ]], dtype=float32)]\n",
            "[array([1., 4., 7.], dtype=float32), array([5., 8.], dtype=float32), array([-0.88], dtype=float32)]\n",
            "[np.float64(16.0), np.float64(25.0), np.float64(31.0)]\n",
            "[np.float64(286.0), np.float64(286.0)]\n",
            "[np.float64(7.699991822242737)]\n",
            "last_layer_output: 7.699991822242737\n",
            "[np.float64(0.999547374076985)]\n",
            "---------------------------- FINAL RESULT ----------------------------\n",
            "[[[np.float64(1.0595201593270445), np.float64(2.119040318654089), np.float64(1.0595201593270445)], [np.float64(-2.8487101591058064), np.float64(-5.697420318211613), np.float64(-2.8487101591058064)], [np.float64(2.119040318654089), np.float64(4.238080637308178), np.float64(2.119040318654089)]], [[np.float64(16.472540267232237), np.float64(25.73834416755037), np.float64(31.915546767762457)], [np.float64(-15.99275798523176), np.float64(-24.988684351924626), np.float64(-30.985968596386535)]], [[np.float64(285.8705489860177), np.float64(285.8705489860177)]]]\n",
            "[[np.float64(1.0595201593270445), np.float64(-2.8487101591058064), np.float64(2.119040318654089)], [np.float64(1.0295337667020148), np.float64(-0.999547374076985)], [np.float64(0.999547374076985)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIn5B4n4j9pZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}